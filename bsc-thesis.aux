\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{pruning_web}
\citation{pruning_arxiv}
\citation{understanding}
\citation{tensorflow2015-whitepaper}
\@writefile{toc}{\contentsline {section}{Tartalmi \IeC {\"o}sszefoglal\IeC {\'o} (Hungarian content summary)}{4}{chapter*.2}}
\citation{applications}
\citation{sobel}
\citation{canny}
\citation{pascal-voc-2012}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Folyamat\IeC {\'a}bra a minimaliz\IeC {\'a}l\IeC {\'a}s m\IeC {\H u}k\IeC {\"o}d\IeC {\'e}s\IeC {\'e}r\IeC {\H o}l\relax }}{6}{figure.caption.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{folyam}{{1}{6}{Folyamatábra a minimalizálás működéséről\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{Task proposal}{9}{chapter*.4}}
\citation{pruning_web}
\citation{pruning_arxiv}
\citation{understanding}
\citation{tensorflow2015-whitepaper}
\@writefile{toc}{\contentsline {section}{Content summary}{10}{chapter*.5}}
\citation{applications}
\@writefile{toc}{\contentsline {section}{Introduction}{11}{chapter*.6}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Technical overview}{13}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Minimal fully convolutional networks}{13}{section.1.1}}
\citation{applications}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Edge detection}{14}{section.1.2}}
\citation{sobel}
\citation{canny}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Conventional algorithmic edge detectors}{15}{subsection.1.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Using the detectors}{15}{subsection.1.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Tools}{16}{section.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Software}{16}{subsection.1.3.1}}
\newlabel{cudnn}{{1.3.1}{17}{Software}{subsection.1.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Hardware}{17}{subsection.1.3.2}}
\citation{pascal-voc-2012}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Implementation}{18}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Code structure}{18}{section.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Preparing the input images}{19}{section.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Network structure}{19}{section.2.3}}
\citation{netscope}
\citation{trunc}
\citation{relu}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The basic structure of the network\relax }}{20}{figure.caption.7}}
\newlabel{net}{{2.1}{20}{The basic structure of the network\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}The objective function and output}{21}{subsection.2.3.1}}
\newlabel{f1loss}{{2.3.1}{21}{The objective function and output}{subsection.2.3.1}{}}
\citation{adam}
\citation{sgd}
\citation{nondet1}
\citation{nondet2}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Training and error back-propagation}{23}{subsection.2.3.2}}
\newlabel{train}{{2.3.2}{23}{Training and error back-propagation}{subsection.2.3.2}{}}
\citation{adam}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Minimization}{24}{section.2.4}}
\newlabel{min}{{2.4}{24}{Minimization}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{Neglecting neurons}{24}{section*.8}}
\@writefile{toc}{\contentsline {subsubsection}{Freezing neurons}{25}{section*.9}}
\@writefile{toc}{\contentsline {subsubsection}{Reinitializing neurons}{25}{section*.10}}
\@writefile{toc}{\contentsline {subsubsection}{Stepping back}{25}{section*.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Testing the worth of a neuron}{26}{subsection.2.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}The minimization process}{26}{subsection.2.4.2}}
\newlabel{thresholds}{{2.4.2}{26}{The minimization process}{subsection.2.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Selection}{27}{subsection.2.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Flowchart of the minimization process\relax }}{27}{figure.caption.12}}
\newlabel{flow}{{2.2}{27}{Flowchart of the minimization process\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{Cosine similarity}{27}{section*.13}}
\@writefile{toc}{\contentsline {subsubsection}{Naive selection strategies}{28}{section*.14}}
\citation{sobel}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Metrics}{29}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Universal Image Quality Index}{29}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Visualization}{29}{section.3.2}}
\newlabel{visualizations}{{3.2}{29}{Visualization}{section.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Mask visualizations, rows on the picture correspond to layers in the network. The first row represents the masks in the first layer. Three groups of five masks from the second layer are omitted.\relax }}{30}{figure.caption.15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Intermediate outputs\relax }}{30}{figure.caption.16}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Ground truth and output examples\relax }}{31}{figure.caption.17}}
\citation{prs}
\citation{knd}
\citation{spr}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Results}{32}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Mask correlations}{32}{section.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces The minimum, maximum, standard deviance, mean and median of the cosine similarities and their absolute values are compared against the relative loss in score and best achieved score after disabling. The comparison uses the Pearson, Kendall and Spearman correlation coefficients\relax }}{33}{table.caption.18}}
\newlabel{correlations}{{4.1}{33}{The minimum, maximum, standard deviance, mean and median of the cosine similarities and their absolute values are compared against the relative loss in score and best achieved score after disabling. The comparison uses the Pearson, Kendall and Spearman correlation coefficients\relax }{table.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Plotting the scores}{33}{section.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Plots displaying the percentage drop and gain in score for various layers. The right column only shows scores that are results of an optimal mask selection.\relax }}{34}{figure.caption.19}}
\newlabel{plots}{{4.1}{34}{Plots displaying the percentage drop and gain in score for various layers. The right column only shows scores that are results of an optimal mask selection.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Minimization strategy charts}{35}{section.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Results of the minimization with specific thresholds and strategies. The groups are the strategies used (CosMax, Random, Sequential), and the bars are corresponding to the score thresholds ($ 85,\ 90,\ 95,\ 98$ and $100\%$)\relax }}{36}{figure.caption.20}}
\newlabel{fin}{{4.2}{36}{Results of the minimization with specific thresholds and strategies. The groups are the strategies used (CosMax, Random, Sequential), and the bars are corresponding to the score thresholds ($ 85,\ 90,\ 95,\ 98$ and $100\%$)\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{37}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\ }
\@writefile{toc}{\contentsline {section}{Nyilatkozat / Statement}{38}{chapter*.21}}
\bibdata{references}
\@writefile{toc}{\contentsline {section}{K\IeC {\"o}sz\IeC {\"o}netnyilv\IeC {\'a}nt\IeC {\'a}s / Acknowledgments}{39}{chapter*.22}}
\bibcite{pruning_web}{1}
\bibcite{pruning_arxiv}{2}
\bibcite{understanding}{3}
\bibcite{tensorflow2015-whitepaper}{4}
\bibcite{applications}{5}
\bibcite{sobel}{6}
\bibcite{canny}{7}
\bibcite{pascal-voc-2012}{8}
\bibcite{netscope}{9}
\bibcite{trunc}{10}
\bibcite{relu}{11}
\bibcite{adam}{12}
\bibcite{sgd}{13}
\bibcite{nondet1}{14}
\bibcite{nondet2}{15}
\bibcite{prs}{16}
\bibcite{knd}{17}
\bibcite{spr}{18}
\bibstyle{unsrt}
