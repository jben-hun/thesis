\begin{thebibliography}{10}

\bibitem{pruning_web}
Jacob Gildenblat.
\newblock Pruning deep neural networks to make them fast and small.
\newblock \url{https://jacobgil.github.io/deeplearning/pruning-deep-learning},
  2016.

\bibitem{pruning_arxiv}
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz.
\newblock Pruning convolutional neural networks for resource efficient
  iference.
\newblock \url{https://arxiv.org/pdf/1611.06440.pdf}, Nov 2016.

\bibitem{understanding}
Ari Morcos and David Barrett.
\newblock Understanding deep learning through neuron deletion.
\newblock
  \url{https://deepmind.com/blog/understanding-deep-learning-through-neuron-deletion/},
  Mar 2018.

\bibitem{tensorflow2015-whitepaper}
Mart\'{\i}n~Abadi et~al.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems.
\newblock \url{http://tensorflow.org/}, 2015.
\newblock Software available from tensorflow.org.

\bibitem{applications}
Convolutional neural network\#{Applications}.
\newblock
  \url{https://en.wikipedia.org/wiki/Convolutional_neural_network#Applications}.

\bibitem{sobel}
Sobel operator.
\newblock \url{https://en.wikipedia.org/wiki/Sobel_operator}.

\bibitem{canny}
Canny edge detector.
\newblock \url{https://en.wikipedia.org/wiki/Canny_edge_detector}.

\bibitem{pascal-voc-2012}
M.~Everingham, L.~Van~Gool, C.~K.~I. Williams, J.~Winn, and A.~Zisserman.
\newblock The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2012
  {(VOC2012)}.
\newblock \url{http://host.robots.ox.ac.uk/pascal/VOC/voc2012/}.

\bibitem{netscope}
Netscope.
\newblock \url{http://ethereon.github.io/netscope/quickstart.html}.

\bibitem{trunc}
Truncated normal distribution.
\newblock \url{https://en.wikipedia.org/wiki/Truncated_normal_distribution}.

\bibitem{relu}
Rectifier (neural networks).
\newblock \url{https://en.wikipedia.org/wiki/Rectifier_(neural_networks)}.

\bibitem{adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \url{https://arxiv.org/pdf/1412.6980.pdf}, Dec 2014.

\bibitem{sgd}
Stochastic gradient descent.
\newblock \url{https://en.wikipedia.org/wiki/Stochastic_gradient_descent}.

\bibitem{nondet1}
Backward pass of broadcasting on gpu is non-deterministic.
\newblock \url{https://github.com/tensorflow/tensorflow/issues/2652}.

\bibitem{nondet2}
Mention that gpu reductions are nondeterministic in docs.
\newblock \url{https://github.com/tensorflow/tensorflow/issues/2732}.

\bibitem{prs}
Pearson correlation coefficient.
\newblock \url{https://en.wikipedia.org/wiki/Pearson_correlation_coefficient}.

\bibitem{knd}
Kendall rank correlation coefficient.
\newblock
  \url{https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient}.

\bibitem{spr}
Spearman's rank correlation coefficient.
\newblock
  \url{https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient}.

\end{thebibliography}
